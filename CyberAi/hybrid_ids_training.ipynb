{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hybrid IDS: Two-Stage Bagging Ensemble with XAI\n",
    "\n",
    "## Architecture\n",
    "- **Bag 1 (Supervised)**: Random Forest + XGBoost\n",
    "- **Bag 2 (Unsupervised)**: Autoencoder + Isolation Forest (boosted by Bag 1)\n",
    "- **XAI Layer**: Fast SHAP-based explanations\n",
    "- **Goal**: Maximize TPR while maintaining FPR < 5%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Our custom modules\n",
    "from autoencoder_trainer import AutoencoderTrainer\n",
    "from isolation_forest_trainer import IsolationForestTrainer\n",
    "from two_stage_ensemble import TwoStageEnsemble\n",
    "from fast_xai_explainer import FastXAIExplainer\n",
    "\n",
    "# Sklearn and other ML libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc\n",
    "import joblib\n",
    "import torch\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "print(\"âœ“ All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CICIDS 2017 dataset (adjust path as needed)\n",
    "print(\"Loading dataset...\")\n",
    "\n",
    "# Assuming you have a combined CSV or you're loading from the existing notebook\n",
    "# This is placeholder code - adjust based on your actual data location\n",
    "data = pd.read_csv('data/cicids_2017_combined.csv')  # Adjust path\n",
    "\n",
    "print(f\"Dataset shape: {data.shape}\")\n",
    "print(f\"\\nColumns: {data.columns.tolist()[:10]}...\")\n",
    "print(f\"\\nLabel distribution:\")\n",
    "print(data['outcome'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and labels\n",
    "# Note: Adjust column names based on your actual dataset\n",
    "label_column = 'outcome'  # or 'Label' depending on your dataset\n",
    "\n",
    "# Separate normal vs attack\n",
    "normal_data = data[data[label_column] == 0].copy()  # Assuming 0 = normal\n",
    "attack_data = data[data[label_column] == 1].copy()  # Assuming 1 = attack\n",
    "\n",
    "print(f\"Normal samples: {len(normal_data):,}\")\n",
    "print(f\"Attack samples: {len(attack_data):,}\")\n",
    "\n",
    "# Handle class imbalance by sampling\n",
    "# For training unsupervised models, we need balanced validation set\n",
    "n_samples = min(len(normal_data), len(attack_data), 100000)  # Limit for speed\n",
    "\n",
    "normal_sampled = normal_data.sample(n=n_samples, random_state=42)\n",
    "attack_sampled = attack_data.sample(n=n_samples, random_state=42)\n",
    "\n",
    "print(f\"\\nSampled {n_samples:,} from each class\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features (drop label column)\n",
    "X_normal = normal_sampled.drop(columns=[label_column]).values\n",
    "X_attack = attack_sampled.drop(columns=[label_column]).values\n",
    "\n",
    "feature_names = normal_sampled.drop(columns=[label_column]).columns.tolist()\n",
    "\n",
    "# Split normal data: training (70%), validation (15%), test (15%)\n",
    "X_normal_train, X_normal_temp = train_test_split(X_normal, test_size=0.3, random_state=42)\n",
    "X_normal_val, X_normal_test = train_test_split(X_normal_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Split attack data: validation (50%), test (50%)\n",
    "X_attack_val, X_attack_test = train_test_split(X_attack, test_size=0.5, random_state=42)\n",
    "\n",
    "print(f\"Normal - Train: {len(X_normal_train):,}, Val: {len(X_normal_val):,}, Test: {len(X_normal_test):,}\")\n",
    "print(f\"Attack - Val: {len(X_attack_val):,}, Test: {len(X_attack_test):,}\")\n",
    "print(f\"\\nFeature count: {len(feature_names)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Training Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize autoencoder trainer\n",
    "ae_trainer = AutoencoderTrainer(\n",
    "    input_dim=len(feature_names),\n",
    "    latent_dim=32,\n",
    "    device='cuda' if torch.cuda.is_available() else 'cpu'\n",
    ")\n",
    "\n",
    "print(f\"Using device: {ae_trainer.device}\")\n",
    "print(f\"Model architecture: {len(feature_names)} â†’ 64 â†’ 48 â†’ 32 (latent) â†’ 48 â†’ 64 â†’ {len(feature_names)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data (scale and create DataLoaders)\n",
    "X_train_ae, X_val_ae = ae_trainer.prepare_data(\n",
    "    X_normal_train,\n",
    "    validation_split=0.2,\n",
    "    batch_size=256\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train autoencoder\n",
    "ae_trainer.train(\n",
    "    epochs=50,\n",
    "    learning_rate=0.001,\n",
    "    patience=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "ae_trainer.plot_training_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize threshold for FPR < 5%\n",
    "ae_threshold, ae_fpr, ae_tpr = ae_trainer.optimize_threshold_for_fpr(\n",
    "    X_normal_val,\n",
    "    X_attack_val,\n",
    "    target_fpr=0.05\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ“ Autoencoder threshold optimized!\")\n",
    "print(f\"  FPR: {ae_fpr*100:.2f}% | TPR: {ae_tpr*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save autoencoder model\n",
    "ae_trainer.save_model('autoencoder_model.pth')\n",
    "print(\"âœ“ Autoencoder model saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training Isolation Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Isolation Forest trainer\n",
    "if_trainer = IsolationForestTrainer()\n",
    "\n",
    "# Prepare data\n",
    "X_train_if, X_val_if = if_trainer.prepare_data(\n",
    "    X_normal_train,\n",
    "    validation_split=0.3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train with grid search\n",
    "best_params_if = if_trainer.train_with_grid_search(\n",
    "    X_train_if,\n",
    "    contamination_range=[0.01, 0.03, 0.05]\n",
    ")\n",
    "\n",
    "print(\"\\nâœ“ Isolation Forest training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize threshold for FPR < 5%\n",
    "if_threshold, if_fpr, if_tpr = if_trainer.optimize_threshold_for_fpr(\n",
    "    X_normal_val,\n",
    "    X_attack_val,\n",
    "    target_fpr=0.05\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ“ Isolation Forest threshold optimized!\")\n",
    "print(f\"  FPR: {if_fpr*100:.2f}% | TPR: {if_tpr*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot score distributions\n",
    "if_trainer.plot_score_distribution(X_normal_val, X_attack_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "if_results = if_trainer.evaluate(X_normal_test, X_attack_test)\n",
    "\n",
    "# Save model\n",
    "if_trainer.save_model('isolation_forest_model.joblib')\n",
    "print(\"âœ“ Isolation Forest model saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Two-Stage Ensemble Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize ensemble\n",
    "ensemble = TwoStageEnsemble()\n",
    "\n",
    "# Load all models\n",
    "ensemble.load_models(\n",
    "    rf_path='random_forest_model.joblib',\n",
    "    xgb_path='xgboost_model_intrusion_detection.joblib',\n",
    "    ae_path='autoencoder_model.pth',\n",
    "    if_path='isolation_forest_model.joblib'\n",
    ")\n",
    "\n",
    "print(\"âœ“ All models loaded into ensemble!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize fusion thresholds for FPR < 5%\n",
    "ensemble_fpr, ensemble_tpr = ensemble.optimize_fusion_thresholds(\n",
    "    X_normal_val,\n",
    "    X_attack_val,\n",
    "    target_fpr=0.05\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ“ Ensemble thresholds optimized!\")\n",
    "print(f\"  Final FPR: {ensemble_fpr*100:.2f}% | TPR: {ensemble_tpr*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save ensemble configuration\n",
    "ensemble.save_ensemble('two_stage_ensemble.joblib')\n",
    "print(\"âœ“ Ensemble configuration saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Comprehensive Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on holdout test set\n",
    "X_test_combined = np.vstack([X_normal_test, X_attack_test])\n",
    "y_test_true = np.hstack([\n",
    "    np.zeros(len(X_normal_test)),\n",
    "    np.ones(len(X_attack_test))\n",
    "])\n",
    "\n",
    "# Get predictions\n",
    "predictions, confidence, all_scores = ensemble.predict(\n",
    "    X_test_combined,\n",
    "    return_all_scores=True\n",
    ")\n",
    "\n",
    "# Convert categories to binary (Normal=0, Attack=1)\n",
    "y_pred = (predictions != \"Normal\").astype(int)\n",
    "\n",
    "print(\"âœ“ Predictions generated on test set!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate metrics\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "accuracy = accuracy_score(y_test_true, y_pred)\n",
    "precision = precision_score(y_test_true, y_pred)\n",
    "recall = recall_score(y_test_true, y_pred)\n",
    "f1 = f1_score(y_test_true, y_pred)\n",
    "\n",
    "# Calculate FPR and TPR\n",
    "tn, fp, fn, tp = confusion_matrix(y_test_true, y_pred).ravel()\n",
    "fpr = fp / (fp + tn)\n",
    "tpr = tp / (tp + fn)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"FINAL ENSEMBLE EVALUATION RESULTS\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Accuracy:  {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "print(f\"Precision: {precision:.4f} ({precision*100:.2f}%)\")\n",
    "print(f\"Recall:    {recall:.4f} ({recall*100:.2f}%)\")\n",
    "print(f\"F1-Score:  {f1:.4f} ({f1*100:.2f}%)\")\n",
    "print(f\"\\nFPR: {fpr*100:.2f}% {'âœ“ MEETS TARGET' if fpr < 0.05 else 'âœ— EXCEEDS TARGET'}\")\n",
    "print(f\"TPR: {tpr*100:.2f}%\")\n",
    "print(f\"\\nConfusion Matrix:\")\n",
    "print(f\"  TN = {tn:,}  |  FP = {fp:,}\")\n",
    "print(f\"  FN = {fn:,}  |  TP = {tp:,}\")\n",
    "print(f\"{'='*60}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize confusion matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "cm_display = ConfusionMatrixDisplay.from_predictions(\n",
    "    y_test_true,\n",
    "    y_pred,\n",
    "    display_labels=['Normal', 'Attack'],\n",
    "    cmap='Blues',\n",
    "    ax=ax\n",
    ")\n",
    "ax.set_title(f'Two-Stage Ensemble Confusion Matrix\\nFPR: {fpr*100:.2f}% | TPR: {tpr*100:.2f}%')\n",
    "plt.tight_layout()\n",
    "plt.savefig('ensemble_confusion_matrix.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC Curve\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "fpr_roc, tpr_roc, thresholds = roc_curve(y_test_true, confidence)\n",
    "roc_auc = auc(fpr_roc, tpr_roc)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(fpr_roc, tpr_roc, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.4f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random')\n",
    "plt.axvline(x=0.05, color='red', linestyle='--', label='FPR Target (5%)')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve - Two-Stage Ensemble')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('ensemble_roc_curve.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. XAI Layer - Explainability Demonstrations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize XAI explainer\n",
    "explainer = FastXAIExplainer()\n",
    "\n",
    "# Load models\n",
    "explainer.load_models(\n",
    "    rf_path='random_forest_model.joblib',\n",
    "    xgb_path='xgboost_model_intrusion_detection.joblib',\n",
    "    feature_names=feature_names\n",
    ")\n",
    "\n",
    "# Initialize SHAP with background data\n",
    "explainer.initialize_shap(\n",
    "    background_data=X_normal_train[:1000],\n",
    "    n_background=100\n",
    ")\n",
    "\n",
    "print(\"âœ“ XAI Explainer initialized!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select diverse examples for explanation\n",
    "# 1. True Positive (correctly detected attack)\n",
    "tp_indices = np.where((y_test_true == 1) & (y_pred == 1))[0]\n",
    "example_tp = tp_indices[0]\n",
    "\n",
    "# 2. False Positive (normal flagged as attack)\n",
    "fp_indices = np.where((y_test_true == 0) & (y_pred == 1))[0]\n",
    "example_fp = fp_indices[0] if len(fp_indices) > 0 else None\n",
    "\n",
    "# 3. True Negative (correctly classified normal)\n",
    "tn_indices = np.where((y_test_true == 0) & (y_pred == 0))[0]\n",
    "example_tn = tn_indices[0]\n",
    "\n",
    "print(f\"Selected examples:\")\n",
    "print(f\"  True Positive (Attack detected): Index {example_tp}\")\n",
    "if example_fp:\n",
    "    print(f\"  False Positive (Normal as Attack): Index {example_fp}\")\n",
    "print(f\"  True Negative (Normal): Index {example_tn}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explain True Positive (Detected Attack)\n",
    "instance_tp = X_test_combined[example_tp]\n",
    "pred_tp = predictions[example_tp]\n",
    "scores_tp = {k: v[example_tp:example_tp+1] for k, v in all_scores.items()}\n",
    "\n",
    "explanation_tp = explainer.explain_instance(\n",
    "    instance_tp,\n",
    "    pred_tp,\n",
    "    scores_tp,\n",
    "    top_n=5\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EXAMPLE 1: TRUE POSITIVE (Attack Correctly Detected)\")\n",
    "print(\"=\"*70)\n",
    "explainer.print_explanation(explanation_tp)\n",
    "explainer.plot_waterfall(explanation_tp, save_path='explanation_true_positive.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explain False Positive (if any)\n",
    "if example_fp:\n",
    "    instance_fp = X_test_combined[example_fp]\n",
    "    pred_fp = predictions[example_fp]\n",
    "    scores_fp = {k: v[example_fp:example_fp+1] for k, v in all_scores.items()}\n",
    "    \n",
    "    explanation_fp = explainer.explain_instance(\n",
    "        instance_fp,\n",
    "        pred_fp,\n",
    "        scores_fp,\n",
    "        top_n=5\n",
    "    )\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"EXAMPLE 2: FALSE POSITIVE (Normal Traffic Flagged as Attack)\")\n",
    "    print(\"=\"*70)\n",
    "    explainer.print_explanation(explanation_fp)\n",
    "    explainer.plot_waterfall(explanation_fp, save_path='explanation_false_positive.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explain True Negative (Normal)\n",
    "instance_tn = X_test_combined[example_tn]\n",
    "pred_tn = predictions[example_tn]\n",
    "scores_tn = {k: v[example_tn:example_tn+1] for k, v in all_scores.items()}\n",
    "\n",
    "explanation_tn = explainer.explain_instance(\n",
    "    instance_tn,\n",
    "    pred_tn,\n",
    "    scores_tn,\n",
    "    top_n=5\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EXAMPLE 3: TRUE NEGATIVE (Normal Traffic)\")\n",
    "print(\"=\"*70)\n",
    "explainer.print_explanation(explanation_tn)\n",
    "explainer.plot_waterfall(explanation_tn, save_path='explanation_true_negative.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest feature importance\n",
    "rf_importance = explainer.get_rf_feature_importance(top_n=15)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "features = [f[0] for f in rf_importance]\n",
    "importances = [f[1] for f in rf_importance]\n",
    "\n",
    "plt.barh(range(len(features)), importances, color='steelblue')\n",
    "plt.yticks(range(len(features)), features)\n",
    "plt.xlabel('Gini Importance')\n",
    "plt.title('Random Forest - Top 15 Feature Importance')\n",
    "plt.grid(True, alpha=0.3, axis='x')\n",
    "plt.tight_layout()\n",
    "plt.savefig('rf_feature_importance.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Summary and Model Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary report\n",
    "summary = f\"\"\"\n",
    "{'='*70}\n",
    "TWO-STAGE BAGGING ENSEMBLE - FINAL SUMMARY\n",
    "{'='*70}\n",
    "\n",
    "MODELS TRAINED:\n",
    "  1. Autoencoder (Unsupervised)\n",
    "     - Architecture: {len(feature_names)} â†’ 64 â†’ 48 â†’ 32 â†’ 48 â†’ 64 â†’ {len(feature_names)}\n",
    "     - FPR: {ae_fpr*100:.2f}% | TPR: {ae_tpr*100:.2f}%\n",
    "  \n",
    "  2. Isolation Forest (Unsupervised)\n",
    "     - Best Params: {best_params_if}\n",
    "     - FPR: {if_fpr*100:.2f}% | TPR: {if_tpr*100:.2f}%\n",
    "  \n",
    "  3. Random Forest + XGBoost (Supervised) - Pre-trained\n",
    "\n",
    "ENSEMBLE PERFORMANCE:\n",
    "  Accuracy:  {accuracy*100:.2f}%\n",
    "  Precision: {precision*100:.2f}%\n",
    "  Recall:    {recall*100:.2f}%\n",
    "  F1-Score:  {f1*100:.2f}%\n",
    "  \n",
    "  FALSE POSITIVE RATE: {fpr*100:.2f}% {'âœ“ MEETS TARGET (<5%)' if fpr < 0.05 else 'âœ— EXCEEDS TARGET'}\n",
    "  TRUE POSITIVE RATE:  {tpr*100:.2f}%\n",
    "  \n",
    "  ROC-AUC: {roc_auc:.4f}\n",
    "\n",
    "SAVED MODELS:\n",
    "  - autoencoder_model.pth\n",
    "  - isolation_forest_model.joblib\n",
    "  - two_stage_ensemble.joblib (configuration)\n",
    "\n",
    "VISUALIZATIONS GENERATED:\n",
    "  - autoencoder_training_history.png\n",
    "  - isolation_forest_scores.png\n",
    "  - ensemble_confusion_matrix.png\n",
    "  - ensemble_roc_curve.png\n",
    "  - rf_feature_importance.png\n",
    "  - explanation_*.png (XAI waterfall plots)\n",
    "\n",
    "{'='*70}\n",
    "\"\"\"\n",
    "\n",
    "print(summary)\n",
    "\n",
    "# Save summary to file\n",
    "with open('training_summary.txt', 'w') as f:\n",
    "    f.write(summary)\n",
    "\n",
    "print(\"\\nâœ“ Summary saved to training_summary.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ðŸŽ‰ TRAINING COMPLETE! ðŸŽ‰\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nAll models trained, optimized, and evaluated.\")\n",
    "print(\"You can now use the ensemble for real-time intrusion detection.\")\n",
    "print(\"\\nNext steps:\")\n",
    "print(\"  1. Deploy models to production environment\")\n",
    "print(\"  2. Integrate with live network traffic capture\")\n",
    "print(\"  3. Build dashboard for real-time monitoring\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
